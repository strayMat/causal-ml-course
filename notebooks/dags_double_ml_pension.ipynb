{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4606f846",
   "metadata": {},
   "source": [
    "# Python: Impact of 401(k) on Financial Wealth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c89d9",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this real-data example, extended from [the Double ML package examples](https://docs.doubleml.org/stable/examples/py_double_ml_pension.html), we will study the effect of 401(k) eligibility and participation on accumulated assets using observational data. More precisely, we will focus on:\n",
    "\n",
    "- Building a causal graph to represent the underlying data-generating process of the and discuss identification assumptions.\n",
    "\n",
    "- introducing how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the effect of 401(k) eligibility and participation on accumulated assets.\n",
    "\n",
    "\n",
    "### Context\n",
    "\n",
    "The 401(k) data set has been analyzed in several studies, among others [Chernozhukov et al. (2018)](https://arxiv.org/abs/1608.00060). 401(k) plans are pension accounts sponsored by employers. The key problem in determining the effect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with the fact that the decision to enroll in a 401(k) is non-random. It is generally recognized that some people have a higher preference for saving than others. It also seems likely that those individuals with high unobserved preference for saving would be most likely to choose to participate in tax-advantaged retirement savings plans and would tend to have otherwise high amounts of accumulated assets. The presence of unobserved savings preferences with these properties then implies that conventional estimates that do not account for saver heterogeneity and endogeneity of participation will be biased upward, tending to overstate the savings effects of 401(k) participation.\n",
    "\n",
    "One can argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income. The basic idea is that, at least around the time 401(k)‚Äôs initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ba34a",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The preprocessed data can be fetched by calling [fetch_401K()](https://docs.doubleml.org/stable/api/generated/doubleml.datasets.fetch_401K.html#doubleml.datasets.fetch_401K). Note that an internet connection is required for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import doubleml as dml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from doubleml.datasets import fetch_401K\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skrub import TableReport\n",
    "from skrub._interpolation_joiner import (\n",
    "    HistGradientBoostingClassifier,\n",
    "    HistGradientBoostingRegressor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef995fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 10.0, 7.5\n",
    "sns.set_style(\n",
    "    \"whitegrid\",\n",
    "    {\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.bottom\": False,\n",
    "        \"axes.spines.left\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "    },\n",
    ")\n",
    "colors = sns.color_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_401K(return_type=\"DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e75175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c97847",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Column descriptions from [hdm R package](https://search.r-project.org/CRAN/refmans/hdm/html/pension.html)\n",
    "\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| nifa | Non-401(k) financial assets |\n",
    "| net_tfa | Net total financial assets (often used as the Outcome Variable) |\n",
    "| tw | Total wealth |\n",
    "| age | Age of the individual |\n",
    "| inc | Annual income (usually in US dollars or thousands) |\n",
    "| fsize | Family size (number of household members) |\n",
    "| educ | Education (measured in years of schooling) |\n",
    "| db | Defined benefit pension (binary: 1 if present, 0 otherwise) |\n",
    "| marr | Marital status (binary: 1 if married, 0 otherwise) |\n",
    "| twoearn | Two-earner household (binary: 1 if both spouses work) |\n",
    "| e401 | Eligibility for 401(k) plan (often the Treatment Variable) |\n",
    "| p401 | Participation in 401(k) plan |\n",
    "| pira | Participation in an Individual Retirement Account (IRA) |\n",
    "| hown | Home ownership (binary: 1 if homeowner, 0 otherwise) |\n",
    "\n",
    "\n",
    "## Skrub TableReport to get a quick overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3590ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TableReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da16dda",
   "metadata": {},
   "source": [
    "The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP).  All the variables are referred to 1990. We use net financial assets (*net\\_tfa*) as the outcome variable, $Y$,  in our analysis. The net financial assets are computed as the sum of IRA balances, 401(k) balances, checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks, and mutual funds less non mortgage debts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ba73d",
   "metadata": {},
   "source": [
    "Among the $9915$ individuals, $3682$ are eligible to participate in the program. The variable *e401* indicates eligibility and *p401* indicates participation, respectively.\n",
    "\n",
    "### üìùTODO: Compute a barplot for the variables 'e401' and 'p401k'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792906cc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Eligibility is highly associated with financial wealth:\n",
    "### üìùTODO: Draw a visualisation that displays clearly this association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716cd5bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "As a first estimate, we calculate the unconditional average predictive effect (APE) of 401(k) eligibility on accumulated assets. This effect would correspond to the average treatment effect if 401(k) eligibility would be assigned to individuals in an entirely randomized way. The unconditional APE of e401 is about $19559$.\n",
    "Among the $3682$ individuals that are eligible, $2594$ decided to participate in the program. The unconditional APE of p401 is about $27372$.\n",
    "\n",
    "### üìùTODO: Compute the naive estimate (randomization case) for the eligibility and the participation variables to recover these naive estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821eda07",
   "metadata": {},
   "source": [
    "As discussed, these estimates are biased since they do not account for saver heterogeneity and endogeneity of participation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal graph representation\n",
    "# ### üìùTODO: Use the `networkx` package to represent the causal graph of the data generating process. Think a little bit about what edges seems reasonable to include or not. To find the full variable labels, you can check the documentation [here](https://search.r-project.org/CRAN/refmans/hdm/html/pension.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the causal relationships\n",
    "# Confounders for e401 : hypothesis is that these variables affect the choice of the employer and thus the eligibility to a 401(k) plan\n",
    "confounders_e401 = []\n",
    "# Confounders for p401 : ie. variables affecting the proportion to save and tus the decision to participate in a 401(k) plan\n",
    "confounders_p401 = [\n",
    "\n",
    "]\n",
    "\n",
    "edges = []\n",
    "\n",
    "for confounder in confounders_e401:\n",
    "    edges.append((confounder, \"e401\"))\n",
    "    edges.append((confounder, \"net_tfa\"))\n",
    "\n",
    "for confounder in confounders_p401:\n",
    "    edges.append((confounder, \"p401\"))\n",
    "    edges.append((confounder, \"net_tfa\"))\n",
    "\n",
    "plt.title(\"A credible DAG for 401(k) Causal Analysis\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c6229",
   "metadata": {},
   "source": [
    "## Using the `DoubleML` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed3b09",
   "metadata": {},
   "source": [
    "Let's use the package [DoubleML](https://docs.doubleml.org/stable/index.html) to estimate the average treatment effect of 401(k) eligibility, i.e. `e401` on net financial assets `net_tfa`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475a05a",
   "metadata": {},
   "source": [
    "## Estimating the Average Treatment Effect of 401(k) Eligibility on Net Financial Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edc8c7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We first look at the treatment effect of `e401` on net total financial assets. We give estimates of the ATE in the Partially Linear Model (PLM)\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Y = D \\theta_0 + g_0(X)+ \\zeta, \\quad E[\\zeta \\mid D,X]= 0,\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where X are the confounders: eg. variables on marital status, two-earner status, defined benefit pension status, IRA participation, home ownership, family size, education, age, and income.\n",
    "\n",
    "We will use only the untransformed variables with linear (lasso) and nonlinear methods, for example random forests or boosting. There are, of course, multiple ways how the statistical models can be specified even more flexibly, for example including interactions of variable and higher order interaction. However, for the sake of simplicity we stick to the specification above. Users who are interested in varying the model can adapt the code below accordingly, for example to implement the original specification in [Chernozhukov et al. (2018)](https://arxiv.org/abs/1608.00060).\n",
    "\n",
    "We will estimate the average treatment effect (ATE) of 401(k) eligibility on net financial assets both in the partially linear regression (PLR) model using different sets of covariates and different machine learning models.\n",
    "The PLR model is implemented in the [DoubleMLPLR](https://docs.doubleml.org/stable/guide/models.html#partially-linear-regression-model-plr) class and can be written as :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y &= D\\theta_0 + g_0(X) + \\zeta, &\\quad E[\\zeta \\mid D,X]= 0,\\\\\n",
    "D &= m_0(X) +  V, &\\quad E[V \\mid X] = 0.\n",
    "\\end{aligned} $$\n",
    "\n",
    "Y is the outcome variable and D is the policy variable of interest. The high-dimensional vector X consists of other confounding covariates, and $\\zeta$ and V are stochastic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e4191",
   "metadata": {},
   "source": [
    "### The Data Backend: `DoubleMLData`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4a39a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "To start our analysis, we initialize the data backend, i.e., a new instance of a [DoubleMLData](https://docs.doubleml.org/dev/api/generated/doubleml.data.DoubleMLData.html#doubleml.data.DoubleMLData) object. We implement the regression model by using scikit-learn's `PolynomialFeatures` class.\n",
    "\n",
    "To implement the different models (basic small, basic full, flexible small and flexible full), we generate different data backends:\n",
    "- `data_dml_small`\n",
    "- `data_dml_full`\n",
    "- `data_dml_flex_small`\n",
    "- `data_dml_flex_full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0748023",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Set up basic model: Specify variables for data-backend\n",
    "features_small = [\n",
    "    \"inc\",\n",
    "    \"age\",\n",
    "    \"educ\",\n",
    "    \"hown\",\n",
    "]\n",
    "features_full = [\n",
    "    *features_small,\n",
    "    \"fsize\",\n",
    "    \"marr\",\n",
    "    \"twoearn\",\n",
    "    \"db\",\n",
    "    \"pira\",\n",
    "]\n",
    "\n",
    "# Initialize DoubleMLData (data-backend of DoubleML)\n",
    "data_dml_small = dml.DoubleMLData(\n",
    "    data, y_col=\"net_tfa\", d_cols=\"e401\", x_cols=features_small\n",
    ")\n",
    "data_dml_full = dml.DoubleMLData(\n",
    "    data, y_col=\"net_tfa\", d_cols=\"e401\", x_cols=features_full\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dml_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cce93f",
   "metadata": {},
   "source": [
    "### Partially Linear Regression Model (PLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212fda0",
   "metadata": {},
   "source": [
    "We start using lasso to estimate the function $g_0$ and $m_0$ in the PLR model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b6ecd",
   "metadata": {},
   "source": [
    "To estimate the causal parameter $\\theta_0$ here, we use double machine learning with 3-fold cross-fitting.\n",
    "\n",
    "Estimation of the nuisance components $g_0$ and $m_0$, is based on the lasso with cross-validated choice of the penalty term , $\\lambda$, as provided by [scikit-learn](https://scikit-learn.org). We load the learner by initializing instances from the classes [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html). Hyperparameters and options can be set during instantiation of the learner. Here we specify that the lasso should use that value of $\\lambda$ that minimizes the cross-validated mean squared error which is based on 5-fold cross validation.\n",
    "\n",
    "We start by estimation the ATE in the basic model and then repeat the estimation in the flexible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b04e3d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Initialize learners\n",
    "### üìùTODO: Instantiate a lasso and a logistic lasso with cross-validated choice of the penalty term. Don't forget to asd a standard scaler in a pipeline.\n",
    "For the lasso coefficients, use a grid of ten values between 0.0001 and 1 (logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed68a9d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "# Initialize DoubleMLPLR model\n",
    "dml_plr_lasso_small = dml.DoubleMLPLR(\n",
    "    data_dml_small, ml_l=lasso, ml_m=lasso_class, n_folds=3\n",
    ")\n",
    "\n",
    "dml_plr_lasso_small.fit(store_predictions=True)\n",
    "lasso_summary_small = dml_plr_lasso_small.summary\n",
    "print(lasso_summary_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the ATE in the full model with lasso\n",
    "\n",
    "np.random.seed(123)\n",
    "dml_plr_lasso_full = dml.DoubleMLPLR(\n",
    "    data_dml_full, ml_l=lasso, ml_m=lasso_class, n_folds=3\n",
    ")\n",
    "\n",
    "dml_plr_lasso_full.fit(store_predictions=True)\n",
    "lasso_summary_full = dml_plr_lasso_full.summary\n",
    "\n",
    "print(lasso_summary_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f23712",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Alternatively, we can repeat this procedure with other machine learning methods, for example a random forest learner as provided by the [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class in [scikit-learn](https://scikit-learn.org).\n",
    "\n",
    " ### üìùTODO: Replicate the previous analysis but using Random Forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be92a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest small model\n",
    "\n",
    "print(forest_summary_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347a4dc",
   "metadata": {
    "lines_to_next_cell": 0,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest full model\n",
    "\n",
    "print(forest_summary_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4eca3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can also experiment with gradient boosting.\n",
    "## üìùTODO: Replicate the previous analysis but using HistGradientBoosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32271e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Boosted Trees small model\n",
    "\n",
    "print(boost_summary_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee320c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Boosted Trees full model\n",
    "\n",
    "print(boost_summary_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065652f",
   "metadata": {},
   "source": [
    "Let's sum up the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plr_summary = pd.concat(\n",
    "    (\n",
    "        lasso_summary_small,\n",
    "        lasso_summary_full,\n",
    "        forest_summary_small,\n",
    "        forest_summary_full,\n",
    "        boost_summary_small,\n",
    "        boost_summary_full,\n",
    "    )\n",
    ")\n",
    "plr_summary.index = [\n",
    "    \"lasso (small)\",\n",
    "    \"lasso (full)\",\n",
    "    \"forest (small)\",\n",
    "    \"forest (full)\",\n",
    "    \"boost (small)\",\n",
    "    \"boost (full)\",\n",
    "]\n",
    "print(plr_summary[[\"coef\", \"2.5 %\", \"97.5 %\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68667793",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "errors = np.full((2, plr_summary.shape[0]), np.nan)\n",
    "errors[0, :] = plr_summary[\"coef\"] - plr_summary[\"2.5 %\"]\n",
    "errors[1, :] = plr_summary[\"97.5 %\"] - plr_summary[\"coef\"]\n",
    "plt.errorbar(plr_summary.index, plr_summary.coef, fmt=\"o\", yerr=errors)\n",
    "plt.ylim([0, 12500])\n",
    "\n",
    "plt.title(\"Partially Linear Regression Model (PLR)\")\n",
    "plt.xlabel(\"ML method\")\n",
    "_ = plt.ylabel(\"Coefficients and 95%-CI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8695db",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Adding model selection for flexible models.\n",
    "In the previous section, we did not perform parameter search for the flexible models. Here, we will illustrate how to perform model selection using the `DoubleML` package and sklearn pipelines.\n",
    "\n",
    "### üìùTODO: Replicate the previous model using a pipeline with randomized search and histgradient boosting for the nuisance models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7a3e2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "üìùTODO What happens if we use a super small set of covariates? Eg. not take into account the income.\n",
    "üìùTODODo you find a minimal set of covariates that you would recommend to absolutely include in the model?\n",
    "### üìùTODO: Try with other sets of covariates for a flexible model and a lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_super_small = [\n",
    "\n",
    "]\n",
    "print(boost_pipe_summary_super_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdee26",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print(lasso_plr_summary_super_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4438f7",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "**Acknowledgement**\n",
    "\n",
    "We would like to thank [Jannis Kueck](https://www.dice.hhu.de/en/dice/people/professors-1/kueck) for sharing [the kaggle notebook](https://www.kaggle.com/janniskueck/pm5-401k). The pension data set has been analyzed in several studies, among others [Chernozhukov et al. (2018)](https://arxiv.org/abs/1608.00060).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-ml-course-practical-sessions",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
