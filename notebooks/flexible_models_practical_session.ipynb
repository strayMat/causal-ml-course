{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a121e7fd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "The aim of the exercise is to get familiar with the histogram gradient-boosting in scikit-learn. Besides, we will use this model within a cross-validation framework in order to inspect internal parameters found via grid-search.\n",
    "\n",
    "We will use the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02ffb5",
   "metadata": {},
   "source": [
    "### üìù TODO: First, create a histogram gradient boosting regressor. You can set the trees number to be large, and configure the model to use early-stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3e839",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907e49e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### üìù TODO: Find appropriate hyperparameters. \n",
    "\n",
    "Use `RandomizedSearchCV` with `n_iter=20` to find the best set of hyperparameters by tuning the following parameters of the `model`:\n",
    "- max_depth: [3, 8];\n",
    "- max_leaf_nodes: [15, 31];\n",
    "- learning_rate: [0.1, 1].\n",
    "\n",
    "Notice that in the notebook \"Hyperparameter tuning by randomized-search\" we\n",
    "pass distributions to be sampled by the `RandomizedSearchCV`. In this case we\n",
    "define a fixed grid of hyperparameters to be explored. Using a `GridSearchCV`\n",
    "instead would explore all the possible combinations on the grid, which can be\n",
    "costly to compute for large grids, whereas the parameter `n_iter` of the\n",
    "`RandomizedSearchCV` controls the number of different random combination that\n",
    "are evaluated. Notice that setting `n_iter` larger than the number of possible\n",
    "combinations in a grid would lead to repeating\n",
    "already-explored combinations. Here, this can't happen since the learning rate is sampled from a uniform so the number of possible combinations is infinite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e610c0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "# CODE HERE\n",
    "param_distributions = \n",
    "\n",
    "model_random_search ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61092d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "üìù TODO: Secondly, we will run our experiment through cross-validation. In this regard, define a 5-fold cross-validation. Besides, be sure to shuffle the data. Subsequently, use the function sklearn.model_selection.cross_validate to run the cross-validation. You should also set return_estimator=True, so that we can investigate the inner model trained via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472e519",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "# CODE HERE\n",
    "cv = \n",
    "cross_val_results ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4e413",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### üìù TODO: Now that we got the cross-validation results, print out the mean and standard deviation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9380e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "cv_mean, cv_sd = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b97d8a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### üìù TODO: Then inspect the estimator entry of the results and check the best parameters values. Besides, check the number of trees used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb6183",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "for estimator in :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43e3b1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "###¬†üìù TODO: Inspect the results of the inner CV for each estimator of the outer CV. Aggregate the mean test score for each parameter combination and make a box plot of these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8979d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# CODE HERE\n",
    "inner_cv_results = []\n",
    "for cv_ix, estimator_ in :\n",
    "    results = \n",
    "    \n",
    "inner_cv_results = pd.concat(inner_cv_results, axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7680750",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the inner_cv_results by the mean of the columns\n",
    "sorted_inner_cv_results = inner_cv_results.mean().sort_values().index\n",
    "inner_cv_results = inner_cv_results[sorted_inner_cv_results]\n",
    "\n",
    "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
    "inner_cv_results.plot.box(vert=False, color=color)\n",
    "plt.xlabel(\"R2 score\")\n",
    "plt.ylabel(\"Parameters\")\n",
    "_ = plt.title(\n",
    "    \"Inner CV results with parameters\\n\" \"(max_depth, max_leaf_nodes, learning_rate)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5df38",
   "metadata": {},
   "source": [
    "We see that the first 4 ranked set of parameters are very close. We could select any of these 4 combinations. It coincides with the results we observe when inspecting the best parameters of the outer CV."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
